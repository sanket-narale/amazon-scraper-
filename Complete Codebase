import logging
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import csv
import pandas as pd
from sqlalchemy import create_engine

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Set up Selenium WebDriver
options = webdriver.ChromeOptions()
options.add_argument("--start-maximized")
driver = webdriver.Chrome(options=options)


def scrape_amazon(search_query, output_file):
    """
    Scrapes product data from Amazon for a given search query.

    Parameters:
        search_query (str): The product search keyword.
        output_file (str): File path to save scraped data.
    """
    try:
        logging.info("Starting the scraping process.")
        # Open Amazon's homepage
        driver.get("https://www.amazon.in/")
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "twotabsearchtextbox")))

        # Search for the specified query
        search_box = driver.find_element(By.ID, "twotabsearchtextbox")
        search_box.send_keys(search_query)
        search_box.send_keys(Keys.RETURN)
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.s-main-slot")))

        # Extract product data
        product_data = []
        products = driver.find_elements(By.CSS_SELECTOR, "div.s-main-slot div.s-result-item")

        logging.info(f"Found {len(products)} products to scrape.")

        for idx, product in enumerate(products):
            product_info = {}

            # Extract Name
            try:
                name = product.find_element(By.CSS_SELECTOR, "h2 a span").text
                product_info["Name"] = name
            except Exception as e:
                logging.debug(f"Failed to extract Name for product {idx}: {e}")
                product_info["Name"] = "Unknown"

            # Extract Price
            try:
                price_whole = product.find_element(By.CSS_SELECTOR, "span.a-price-whole").text
                price_fraction = product.find_element(By.CSS_SELECTOR, "span.a-price-fraction").text
                product_info["Price"] = f"{price_whole}.{price_fraction}"
            except Exception as e:
                logging.debug(f"Failed to extract Price for product {idx}: {e}")
                product_info["Price"] = "Unknown"

            # Extract Rating
            try:
                rating = product.find_element(By.CSS_SELECTOR, "span.a-icon-alt").text
                product_info["Rating"] = rating
            except Exception as e:
                logging.debug(f"Failed to extract Rating for product {idx}: {e}")
                product_info["Rating"] = "Unknown"

            # Extract Reviews
            try:
                reviews = product.find_element(By.CSS_SELECTOR, "span.a-size-base").text
                product_info["Reviews"] = reviews
            except Exception as e:
                logging.debug(f"Failed to extract Reviews for product {idx}: {e}")
                product_info["Reviews"] = "Unknown"

            logging.info(f"Product {idx + 1}: {product_info}")
            product_data.append(product_info)

        # Save data to CSV
        with open(output_file, "w", newline="", encoding="utf-8") as csvfile:
            fieldnames = ["Name", "Price", "Rating", "Reviews"]
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(product_data)

        logging.info(f"Data scraped successfully and saved to {output_file}")

        # Preprocess data
        preprocess_data(output_file)

        # Store data in SQLite
        store_data_in_db(output_file)

    except Exception as e:
        logging.error(f"An error occurred during scraping: {e}")

    finally:
        driver.quit()


def preprocess_data(input_file):
    """
    Cleans and preprocesses scraped data.

    Parameters:
        input_file (str): File path of the scraped data.
    """
    try:
        logging.info("Starting data preprocessing.")
        # Load data into a DataFrame
        df = pd.read_csv(input_file)

        # Handle missing values
        df.fillna("Unknown", inplace=True)

        # Normalize Price column
        df["Price"] = pd.to_numeric(df["Price"].str.replace(",", ""), errors="coerce")

        # Normalize Ratings column
        df["Rating"] = pd.to_numeric(df["Rating"].str.extract(r"([0-9.]+)"), errors="coerce")

        # Convert Reviews to numeric
        df["Reviews"] = pd.to_numeric(df["Reviews"], errors="coerce")

        # Save the cleaned data back to CSV
        df.to_csv(input_file, index=False)
        logging.info(f"Data cleaned and saved to {input_file}")
    except Exception as e:
        logging.error(f"An error occurred during preprocessing: {e}")


def store_data_in_db(input_file):
    """
    Stores the cleaned data into a SQLite database.

    Parameters:
        input_file (str): File path of the cleaned data.
    """
    try:
        logging.info("Storing data in SQLite database.")
        # Load the data
        df = pd.read_csv(input_file)

        # Connect to SQLite database
        engine = create_engine("sqlite:///amazon_products.db")
        df.to_sql("products", con=engine, if_exists="replace", index=False)
        logging.info("Data stored successfully in SQLite database.")
    except Exception as e:
        logging.error(f"An error occurred while storing data in the database: {e}")


if __name__ == "__main__":
    try:
        logging.info("Script started.")
        scrape_amazon("laptops", "amazon_products.csv")
        logging.info("Script completed successfully.")
    except Exception as e:
        logging.error(f"An error occurred in the main script: {e}")
